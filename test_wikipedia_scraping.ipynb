{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrape.wp_get_page import WikipediaPage\n",
    "from scrape.wp_structured_text import WikipediaStructuredText\n",
    "import difflib\n",
    "from setup.db_trie import db\n",
    "from lakat.submit import content_submit\n",
    "from config.scrape_cfg import WIKIPEDIA_API_URL, EXAMPLE_ARTICLE_TITLE\n",
    "from config.bucket_cfg import DEFAULT_ATOMIC_BUCKET_SCHEMA, DEFAULT_MOLECULAR_BUCKET_SCHEMA, DEFAULT_NAME_RESOLUTION_BUCKET_SCHEMA, BUCKET_ID_TYPE_NO_REF\n",
    "from utils.signing.sign import get_public_key_from_file\n",
    "from utils.serialize import unserialize, serialize\n",
    "\n",
    "# db.close()\n",
    "article_title = EXAMPLE_ARTICLE_TITLE\n",
    "wp = WikipediaPage(WIKIPEDIA_API_URL)\n",
    "edit_history = wp.load_content_from_batches(\n",
    "    article_title, 0, 105, download_if_not_exist=True)\n",
    "edit_history.reverse()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_file_prefix=\"lakat\"\n",
    "    # retrieve\n",
    "public_key = get_public_key_from_file(key_file_prefix=key_file_prefix)\n",
    "# create a name registry NR (optional)\n",
    "contents = list()\n",
    "data_dict = {\n",
    "        \"schema_id\": DEFAULT_NAME_RESOLUTION_BUCKET_SCHEMA,\n",
    "        \"public_key\": public_key,\n",
    "        \"parent_bucket\": None,\n",
    "        \"data\": serialize({}),\n",
    "        \"refs\": serialize([])\n",
    "    }\n",
    "contents.append(serialize(data_dict))\n",
    "\n",
    "\n",
    "# socialRefs = [{\"id\": someId, \"value\":someValue}]\n",
    "socialRefs = list()\n",
    "interactions = {\n",
    "    \"socialRefs\": socialRefs,\n",
    "    \"reviews\": list(),\n",
    "    \"tokens\": list(),\n",
    "    \"bucketRefs\": list(),\n",
    "    \"storageProofs\": list(),\n",
    "    \"children\": list()}\n",
    "\n",
    "res = content_submit(\n",
    "        contents=contents,\n",
    "        interactions=interactions, \n",
    "        branchId=None, \n",
    "        proof=b'', \n",
    "        msg=\"NAME REGISTRY AND INITIAL SUBMIT\", \n",
    "        create_branch=True)\n",
    "for id, type_of_content in res.items():\n",
    "    if type_of_content == \"BRANCH\":\n",
    "        branchId = id\n",
    "        break\n",
    "# print(\"branchId: \", branchId)\n",
    "# print(\"res: \", res)\n",
    "\n",
    "edit = edit_history[0]\n",
    "structured_text_old = WikipediaStructuredText(edit[\"*\"])\n",
    "msg = edit[\"comment\"]\n",
    "old_parts = structured_text_old.parts\n",
    "contents = list()\n",
    "order = list()\n",
    "for i, part in enumerate(old_parts):\n",
    "    data_dict = {\n",
    "        \"schema_id\": DEFAULT_ATOMIC_BUCKET_SCHEMA,\n",
    "        \"public_key\": public_key,\n",
    "        \"parent_bucket\": None,\n",
    "        \"data\": serialize(part.content),\n",
    "        \"refs\": serialize([])\n",
    "    }\n",
    "    contents.append(serialize(data_dict))\n",
    "    order.append(i)\n",
    "\n",
    "# # create a molecular bucket\n",
    "molecular_data = {\n",
    "    \"order\":[{\"id\": oid, \"type\": BUCKET_ID_TYPE_NO_REF} for oid in order],\n",
    "    \"name\": EXAMPLE_ARTICLE_TITLE}\n",
    "\n",
    "data_dict = {\n",
    "    \"schema_id\": DEFAULT_MOLECULAR_BUCKET_SCHEMA,\n",
    "    \"public_key\": public_key,\n",
    "    \"parent_bucket\": None,\n",
    "    \"data\": serialize(molecular_data),\n",
    "    \"refs\": serialize([])\n",
    "}\n",
    "contents.append(serialize(data_dict))\n",
    "\n",
    "res = content_submit(\n",
    "        contents=contents, \n",
    "        interactions=interactions,\n",
    "        branchId=branchId, \n",
    "        proof=b'', \n",
    "        msg=msg, \n",
    "        create_branch=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'QmSHtwJV1w6rKH3CAgpGYPVf5F16vyFfGGEt2xudjWSxXC': 'ATOMIC',\n",
       " 'QmYhjeyL4cXmtxGGMsJqfucq87c5FcbvqNdxYMUFNwdrZq': 'ATOMIC',\n",
       " 'QmNviPBkGzzCWEhhuDyXEbV1NwER8GbXYpvAXgTvoWDpkV': 'ATOMIC',\n",
       " 'QmVi1VysfHc5KNAPmTYJ72rFW77EP4FjdJxcVDaxca7kGg': 'ATOMIC',\n",
       " 'Qmf2og8L8GyvdAmXVmozhgzPi62uqMB5sHNfZpRV2v5MqR': 'ATOMIC',\n",
       " 'QmSi5Y9C1aRokVKRZ4WDKA78Zc3ArArEnmaGH8Uma9joXe': 'ATOMIC',\n",
       " 'QmaEzXn3knZZbQ6iL6TJuWtidFfk3zLEexhVkRC5vDiSXs': 'ATOMIC',\n",
       " 'QmZmK7RfJuFPKY5U2tXiy31sQyQNcLxvjPtK344oaZ7QJj': 'ATOMIC',\n",
       " 'QmWLpd2qK5qeC1A5oEKRM3uqp4x8cf47EdKxWi3sFnNEmM': 'ATOMIC',\n",
       " 'QmUhPKgizvzG6zxPYdq8izoNkTbHDyWafy1V2xQdjHHdHB': 'ATOMIC',\n",
       " 'QmSCQ3wPf7nhrguyy9Gbn5YGrBfKpcVSuHcm1PizuPaKLu': 'SUBMIT_TRACE',\n",
       " 'QmUQ3mLexWRjADXzSqmuyHjFUG95QgzPJifiBbYDZ2yRmD': 'SUBMIT',\n",
       " 'QmahMPQdDWvU9Y7YwUsHuystuEfterDrPHh7sbR3fUjfhS': 'BRANCH'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10: 500 revisions\n",
      "Batch 17: 338 revisions\n"
     ]
    }
   ],
   "source": [
    "index = wp.save_edit_history(title=TITLE, limit=None, requests_limit=8000, batch_size=10, api_sleep_in_sec=2, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_content = wp.load_content_from_batches(TITLE, 0, 105)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = [{k:str(v)[0:min(20,len(str(v)))] for k,v in c.items()} for c in loaded_content]\n",
    "# d[3:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'deleted': [{'part': {'title': 'Pandemic', 'super_title': 'Variations on the basic SIR mo', 'level': '3', 'header': 'Pandemic', 'headerless_content': 'An SIR community based model t', 'part_id': '26'},\n",
       "   'max_similarity': 0.05374823196605375,\n",
       "   'new_index': 25}],\n",
       " 'added': [],\n",
       " 'rearranged': [{'old_index': 27, 'new_index': 26, 'score': 1},\n",
       "  {'old_index': 28, 'new_index': 27, 'score': 1},\n",
       "  {'old_index': 29, 'new_index': 28, 'score': 1},\n",
       "  {'old_index': 30, 'new_index': 29, 'score': 1},\n",
       "  {'old_index': 31, 'new_index': 30, 'score': 1},\n",
       "  {'old_index': 32, 'new_index': 31, 'score': 1},\n",
       "  {'old_index': 33, 'new_index': 32, 'score': 1},\n",
       "  {'old_index': 34, 'new_index': 33, 'score': 1},\n",
       "  {'old_index': 35, 'new_index': 34, 'score': 1},\n",
       "  {'old_index': 36, 'new_index': 35, 'score': 1},\n",
       "  {'old_index': 37, 'new_index': 36, 'score': 1},\n",
       "  {'old_index': 38, 'new_index': 37, 'score': 1},\n",
       "  {'old_index': 39, 'new_index': 38, 'score': 1},\n",
       "  {'old_index': 40, 'new_index': 39, 'score': 1},\n",
       "  {'old_index': 41, 'new_index': 40, 'score': 1}],\n",
       " 'modified': []}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scrape.wp_structured_diffs import Diff, _similar_content\n",
    "from scrape.wp_structured_text import WikipediaStructuredText\n",
    "\n",
    "ind = 152\n",
    "str_text_old = WikipediaStructuredText(loaded_content[ind][\"*\"])\n",
    "str_text_new = WikipediaStructuredText(loaded_content[ind+1][\"*\"])\n",
    "diff = Diff(old=str_text_old, new=str_text_new)\n",
    "df = diff.get_diff(similarity_threshold=0.75, zero_level_similarity_threshold=0.95)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each loaded content has a revid and a parentid. could you make a networkx object from loaded_content? Can you check whether its a chain or something non-trivial?\n",
    "# import networkx as nx\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# G = nx.DiGraph()\n",
    "# for c in loaded_content:\n",
    "#     G.add_node(c[\"revid\"], parentid=c[\"parentid\"], user=c[\"user\"], timestamp=c[\"timestamp\"], comment=c[\"comment\"], revid=c[\"revid\"])\n",
    "#     if c[\"parentid\"] != 0:\n",
    "#         G.add_edge(c[\"parentid\"], c[\"revid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # make the nodes a lot smaller\n",
    "# node_size = [1 for n in G.nodes()]\n",
    "\n",
    "# # get the nodes with more than one children?\n",
    "# # get the adjacency matrix and transpose it and \n",
    "# A = nx.adjacency_matrix(G)\n",
    "# # max(A.sum(axis=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'revid': 16745668,\n",
       " 'parentid': 0,\n",
       " 'user': 'Imoen',\n",
       " 'timestamp': '2004-09-04T18:24:28Z',\n",
       " 'contentformat': 'text/x-wiki',\n",
       " 'contentmodel': 'wikitext',\n",
       " 'comment': 'Shiny new page',\n",
       " '*': \"A [[population]] comprises a large number of individuals, all of whom are different in various fields. In order to [[mathematical modelling in epidemiology|model the progress of an epidemic]] in such a population this diversity must be reduced to a few key characteristics which are relevant to the infection under consideration. For example, for most common childhood diseases which confer long-lasting immunity it makes sense to divide the population into those who are [[susceptible]] to the disease, those who are [[infectious disease|infected]] and those who have recovered and are [[immune system|immune]]. These subdivisions of the population are called '''compartments'''.\\n\\n==The SIR model==\\n\\nStandard convention labels these three compartments S (for susceptible), I (for infectious) and R (for recovered). Therefore, this model is called the SIR model. \\n\\nThis is a good, simple, model for many infectious diseases including [[measles]], [[mumps]] and [[rubella]].\\n\\nThe letters also represent the number of people in each compartment at a particular time. To indicate that the numbers might vary over time (even if the total population size remains constant), we make the precise numbers a function of ''t'' (time): S(''t''), I(''t'') and R(''t''). For a specific disease in a specific population, these functions may be worked out in order to predict possible outbreaks and bring them under control.\\n\\n===The SIR model is dynamic in two senses===\\n\\nAs implied by the variable function of ''t'', the model is dynamic in that the numbers in each compartment may fluctuate over time. The importance of this dynamic aspect is most obvious in an [[endemic (epidemiology)|endemic]] disease with a short infectious period, such as [[measles]] in the UK prior to the introduction of a [[vaccination|vaccine]] in [[1968]]. Such diseases tend to occur in cycles of outbreaks due to the variation in number of susceptibles (S(''t'')) over time. During an [[epidemic]], the number of susceptibles falls rapidly as more of them are infected and thus enter the infectious and recovered compartments. The disease cannot break out again until the number of susceptibles has built back up as a result of babies being born into the compartment.\\n\\nThe SIR is also dynamic in the sense that individuals are born susceptible, then may acquire the infection (move into the infectious compartment) and finally recover (move into the recovered compartment). Thus each member of the population typically progresses from susceptible to infectious to recovered. This can be shown as a flow diagram in which the boxes represent the different compartments and the arrows the transition between compartments.\\n\\n[[Image:SIR.PNG|800px|center|SIR compartmental model]]\\n\\n===Transition rates===\\n\\nFor the full specification of the model, the arrows should be labelled with the transition rates between compartments.\\n\\nBetween S and I, the transition rate is &lambda;, the [[force of infection]], which is simply the rate at which susceptible individuals become infected by an infectious disease.\\n\\nBetween I and R, the transition rate is &delta; (simply the rate of recovery). If the duration of the infection is denoted ''D'', then &delta; = 1/''D'', since an individual experiences one recovery in ''D'' units of time.\\n\\n==Elaborations on the basic SIR model==\\n\\n===The SEIR model===\\n\\nFor many infections there is a period of time during which the individual has been infected but is not yet infectious himself. During this latent period the individual is in compartment E (for exposed).\\n\\n[[Image:SEIR.PNG|800px|center|SEIR compartmental model]]\\n\\n===The MSIR model===\\n\\nFor many infections, including [[measles]], babies are not born into the susceptible compartment but are immune to the disease for the first few months of life due to protection from maternal antibodies (passed across the [[placenta]] or through [[colostrum]]). This added detail can be shown by including an M class (for maternally derived immunity) at the beginning of the model.\\n\\n[[Image:MSIR.PNG|800px|center|MSIR compartmental model]]\\n\\n===Carrier state===\\n\\nSome people who have had an infectious disease such as [[tuberculosis]] never completely recover and continue to [[asymptomatic carrier|carry]] the infection, whilst not suffering the disease themselves. They may then move back into the infectious compartment and suffer symptoms (as in tuberculosis) or they may continue to infect others in their carrier state, while not suffering symptoms. The most famous example of this is probably [[Mary Mallon]], who infected 22 people with [[typhoid fever]]. The carrier compartment is labelled C.\\n\\n[[Image:SIR_model_with_carrier.PNG|800px|center|SIR compartmental model with carrier class, C]]\\n\\n==The SIS model==\\n\\nSome infections, for example the group of those responsible for the [[common cold]], do not confer any long lasting immunity. Such infections do not have a recovered state and individuals become susceptible again after infection.\\n\\n[[Image:SIS.PNG|800px|center|SIS compartmental model]]\\n\\n==See also==\\n*[[Mathematical modelling in epidemiology]]\\n\\n[[Category:Epidemiology]]\"}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "loaded_content[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 {'deleted': [], 'added': [], 'rearranged': [], 'modified': [{'old_index': 17, 'new_index': 17, 'score': 1}]} \n",
      "\n",
      "\n",
      "1 2 {'deleted': [], 'added': [], 'rearranged': [], 'modified': [{'old_index': 9, 'new_index': 9, 'score': 1}]} \n",
      "\n",
      "\n",
      "2 3 {'deleted': [], 'added': [], 'rearranged': [], 'modified': [{'old_index': 8, 'new_index': 8, 'score': 1}]} \n",
      "\n",
      "\n",
      "3 4 {'deleted': [], 'added': [], 'rearranged': [], 'modified': [{'old_index': 9, 'new_index': 9, 'score': 1}]} \n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m str_text_new \u001b[39m=\u001b[39m WikipediaStructuredText(loaded_content[i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m      7\u001b[0m diff \u001b[39m=\u001b[39m Diff(old\u001b[39m=\u001b[39mstr_text_old, new\u001b[39m=\u001b[39mstr_text_new)\n\u001b[0;32m----> 8\u001b[0m df \u001b[39m=\u001b[39m diff\u001b[39m.\u001b[39;49mget_diff(similarity_threshold\u001b[39m=\u001b[39;49m\u001b[39m0.75\u001b[39;49m, zero_level_similarity_threshold\u001b[39m=\u001b[39;49m\u001b[39m0.95\u001b[39;49m)\n\u001b[1;32m      9\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(df) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     10\u001b[0m     \u001b[39mprint\u001b[39m(i, i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, df, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Projects/Lakat/lakat-py/scrape/wp_structured_diffs.py:19\u001b[0m, in \u001b[0;36mDiff.get_diff\u001b[0;34m(self, similarity_threshold, zero_level_similarity_threshold)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_diff\u001b[39m(\u001b[39mself\u001b[39m, similarity_threshold\u001b[39m=\u001b[39m\u001b[39m0.6\u001b[39m, zero_level_similarity_threshold\u001b[39m=\u001b[39m\u001b[39m0.9\u001b[39m):\n\u001b[1;32m     18\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the diff between the old and new text.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     \u001b[39mreturn\u001b[39;00m analyze_differences(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mold\u001b[39m.\u001b[39;49mparts, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnew\u001b[39m.\u001b[39;49mparts, similarity_threshold\u001b[39m=\u001b[39;49msimilarity_threshold, \n\u001b[1;32m     20\u001b[0m     zero_level_similarity_threshold\u001b[39m=\u001b[39;49mzero_level_similarity_threshold)\n",
      "File \u001b[0;32m~/Projects/Lakat/lakat-py/scrape/wp_structured_diffs.py:114\u001b[0m, in \u001b[0;36manalyze_differences\u001b[0;34m(text1_parts, text2_parts, similarity_threshold, zero_level_similarity_threshold)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m# Identify deleted and added parts\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m text1_parts:\n\u001b[0;32m--> 114\u001b[0m     q, i, score \u001b[39m=\u001b[39m find_similar_part_in_second(p, text2_parts, similarity_threshold\u001b[39m=\u001b[39;49msimilarity_threshold, \n\u001b[1;32m    115\u001b[0m     zero_level_similarity_threshold\u001b[39m=\u001b[39;49mzero_level_similarity_threshold)\n\u001b[1;32m    116\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m q:\n\u001b[1;32m    117\u001b[0m         deleted\u001b[39m.\u001b[39mappend({\u001b[39m\"\u001b[39m\u001b[39mpart\u001b[39m\u001b[39m\"\u001b[39m:p, \u001b[39m\"\u001b[39m\u001b[39mmax_similarity\u001b[39m\u001b[39m\"\u001b[39m: score, \u001b[39m\"\u001b[39m\u001b[39mnew_index\u001b[39m\u001b[39m\"\u001b[39m: i})\n",
      "File \u001b[0;32m~/Projects/Lakat/lakat-py/scrape/wp_structured_diffs.py:87\u001b[0m, in \u001b[0;36mfind_similar_part_in_second\u001b[0;34m(part_1, part_2_s, similarity_threshold, zero_level_similarity_threshold)\u001b[0m\n\u001b[1;32m     85\u001b[0m most_similar_part \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39mFalse\u001b[39;00m]\n\u001b[1;32m     86\u001b[0m \u001b[39mfor\u001b[39;00m i, other_part \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(part_2_s):\n\u001b[0;32m---> 87\u001b[0m     flag, sim_score \u001b[39m=\u001b[39m is_similar(part_1, other_part, similarity_threshold\u001b[39m=\u001b[39;49msimilarity_threshold,\n\u001b[1;32m     88\u001b[0m     zero_level_similarity_threshold\u001b[39m=\u001b[39;49mzero_level_similarity_threshold)\n\u001b[1;32m     89\u001b[0m     \u001b[39mif\u001b[39;00m sim_score \u001b[39m>\u001b[39m most_similar_part[\u001b[39m2\u001b[39m]:\n\u001b[1;32m     90\u001b[0m         most_similar_part \u001b[39m=\u001b[39m [other_part, i, sim_score, flag]\n",
      "File \u001b[0;32m~/Projects/Lakat/lakat-py/scrape/wp_structured_diffs.py:51\u001b[0m, in \u001b[0;36mis_similar\u001b[0;34m(part1, part2, similarity_threshold, zero_level_similarity_threshold)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m         \u001b[39m# TODO: maybe change this\u001b[39;00m\n\u001b[1;32m     49\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m, \u001b[39m1\u001b[39m\n\u001b[0;32m---> 51\u001b[0m sim_thr \u001b[39m=\u001b[39m _similar_content(part1\u001b[39m.\u001b[39;49mcontent, part2\u001b[39m.\u001b[39;49mcontent)\n\u001b[1;32m     52\u001b[0m \u001b[39m# Check if titles are different but contents are similar\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[39mif\u001b[39;00m part1\u001b[39m.\u001b[39mtitle \u001b[39m!=\u001b[39m part2\u001b[39m.\u001b[39mtitle \u001b[39mand\u001b[39;00m part1\u001b[39m.\u001b[39msuper_title \u001b[39m==\u001b[39m part2\u001b[39m.\u001b[39msuper_title:\n\u001b[1;32m     54\u001b[0m     \u001b[39m# if part1.level == 0:\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \u001b[39m#     return sim_thr >= zero_level_similarity_threshold, sim_thr\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/Lakat/lakat-py/scrape/wp_structured_diffs.py:80\u001b[0m, in \u001b[0;36m_similar_content\u001b[0;34m(content1, content2)\u001b[0m\n\u001b[1;32m     77\u001b[0m matcher \u001b[39m=\u001b[39m difflib\u001b[39m.\u001b[39mSequenceMatcher(\u001b[39mNone\u001b[39;00m, content1, content2)\n\u001b[1;32m     79\u001b[0m \u001b[39m# The ratio is a measure of the sequences' similarity\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m \u001b[39mreturn\u001b[39;00m matcher\u001b[39m.\u001b[39;49mratio()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/lib/python3.9/difflib.py:619\u001b[0m, in \u001b[0;36mSequenceMatcher.ratio\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mratio\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    598\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return a measure of the sequences' similarity (float in [0,1]).\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \n\u001b[1;32m    600\u001b[0m \u001b[39m    Where T is the total number of elements in both sequences, and\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[39m    1.0\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 619\u001b[0m     matches \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(triple[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m triple \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_matching_blocks())\n\u001b[1;32m    620\u001b[0m     \u001b[39mreturn\u001b[39;00m _calculate_ratio(matches, \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ma) \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/lib/python3.9/difflib.py:454\u001b[0m, in \u001b[0;36mSequenceMatcher.get_matching_blocks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[39mwhile\u001b[39;00m queue:\n\u001b[1;32m    453\u001b[0m     alo, ahi, blo, bhi \u001b[39m=\u001b[39m queue\u001b[39m.\u001b[39mpop()\n\u001b[0;32m--> 454\u001b[0m     i, j, k \u001b[39m=\u001b[39m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfind_longest_match(alo, ahi, blo, bhi)\n\u001b[1;32m    455\u001b[0m     \u001b[39m# a[alo:i] vs b[blo:j] unknown\u001b[39;00m\n\u001b[1;32m    456\u001b[0m     \u001b[39m# a[i:i+k] same as b[j:j+k]\u001b[39;00m\n\u001b[1;32m    457\u001b[0m     \u001b[39m# a[i+k:ahi] vs b[j+k:bhi] unknown\u001b[39;00m\n\u001b[1;32m    458\u001b[0m     \u001b[39mif\u001b[39;00m k:   \u001b[39m# if k is 0, there was no matching block\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.15/lib/python3.9/difflib.py:388\u001b[0m, in \u001b[0;36mSequenceMatcher.find_longest_match\u001b[0;34m(self, alo, ahi, blo, bhi)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[39mif\u001b[39;00m k \u001b[39m>\u001b[39m bestsize:\n\u001b[1;32m    387\u001b[0m             besti, bestj, bestsize \u001b[39m=\u001b[39m i\u001b[39m-\u001b[39mk\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, j\u001b[39m-\u001b[39mk\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, k\n\u001b[0;32m--> 388\u001b[0m     j2len \u001b[39m=\u001b[39m newj2len\n\u001b[1;32m    390\u001b[0m \u001b[39m# Extend the best by non-junk elements on each end.  In particular,\u001b[39;00m\n\u001b[1;32m    391\u001b[0m \u001b[39m# \"popular\" non-junk elements aren't in b2j, which greatly speeds\u001b[39;00m\n\u001b[1;32m    392\u001b[0m \u001b[39m# the inner loop above, but also means \"the best\" match so far\u001b[39;00m\n\u001b[1;32m    393\u001b[0m \u001b[39m# doesn't contain any junk *or* popular non-junk elements.\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \u001b[39mwhile\u001b[39;00m besti \u001b[39m>\u001b[39m alo \u001b[39mand\u001b[39;00m bestj \u001b[39m>\u001b[39m blo \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    395\u001b[0m       \u001b[39mnot\u001b[39;00m isbjunk(b[bestj\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]) \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    396\u001b[0m       a[besti\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m b[bestj\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# str_text_old.parts[4].headerless_content\n",
    "# _similar_content(str_text_old.parts[4].content, str_text_new.parts[2].content)\n",
    "# loaded_content[0]\n",
    "for i in range(0, len(loaded_content)-1):\n",
    "    str_text_old = WikipediaStructuredText(loaded_content[i][\"*\"])\n",
    "    str_text_new = WikipediaStructuredText(loaded_content[i+1][\"*\"])\n",
    "    diff = Diff(old=str_text_old, new=str_text_new)\n",
    "    df = diff.get_diff(similarity_threshold=0.75, zero_level_similarity_threshold=0.95)\n",
    "    if len(df) > 0:\n",
    "        print(i, i+1, df, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['last1=Ross |first1=Ronald |title=An application of the theory of probabilities to the study of a priori pathometry.—Part I |journal=  Proceedings of the Royal Society of London. Series A, Containing Papers of a Mathematical and Physical Character|date=1 February 1916 |volume=92 |issue=638 |pages=204–230 |doi=10.1098/rspa.1916.0007|bibcode=1916RSPSA..92..204R |doi-access=free ',\n",
       " 'last1=Ross |first1=Ronald |last2=Hudson |first2=Hilda |title=An application of the theory of probabilities to the study of a priori pathometry.—Part II |journal= Proceedings of the Royal Society of London. Series A, Containing Papers of a Mathematical and Physical Character|date=3 May 1917 |volume=93 |issue=650 |pages = 212–225 |doi=10.1098/rspa.1917.0014 |bibcode=1917RSPSA..93..212R |url=https://royalsocietypublishing.org/doi/10.1098/rspa.1917.0014|doi-access=free ',\n",
       " 'last1=Ross |first1=Ronald |last2=Hudson |first2=Hilda |title=An application of the theory of probabilities to the study of a priori pathometry.—Part III |journal= Proceedings of the Royal Society of London. Series A, Containing Papers of a Mathematical and Physical Character|date=1917 |volume=89 |issue=621 |pages=225–240 |doi=10.1098/rspa.1917.0015 |bibcode=1917RSPSA..93..225R |url=https://royalsocietypublishing.org/doi/10.1098/rspa.1917.0015|doi-access=free ',\n",
       " 'last1=Kermack |first1=W. O. |last2=McKendrick |first2=A. G. |title=A Contribution to the Mathematical Theory of Epidemics |journal= Proceedings of the Royal Society of London. Series A, Containing Papers of a Mathematical and Physical Character|volume=115 |issue=772 |pages=700–721 |date=1927 |doi=10.1098/rspa.1927.0118|bibcode=1927RSPSA.115..700K |doi-access=free ']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the diff using difflib library of loaded_content[ind][\"*\"] and loaded_content[ind + 1][\"*\"]\n",
    "# differencee = difflib.ndiff(loaded_content[ind][\"*\"].splitlines(keepends=True), loaded_content[ind + 1][\"*\"].splitlines(keepends=True))\n",
    "# list(differencee)\n",
    "# [m[1].content for m in df.get('modified')]\n",
    "str_text_new.parts[3].content\n",
    "print(str_text_old.parts[3].level)\n",
    "# _similar_content(str_text_old.parts[5].content, str_text_new.parts[2].content)\n",
    "[p.content for p in str_text_old.parts[1:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['--- ',\n",
       " '+++ ',\n",
       " '@@ -901,23 +901,6 @@',\n",
       " ' -',\n",
       " ' 0',\n",
       " ' 2',\n",
       " '-|',\n",
       " '-d',\n",
       " '-o',\n",
       " '-i',\n",
       " '--',\n",
       " '-a',\n",
       " '-c',\n",
       " '-c',\n",
       " '-e',\n",
       " '-s',\n",
       " '-s',\n",
       " '-=',\n",
       " '-f',\n",
       " '-r',\n",
       " '-e',\n",
       " '-e',\n",
       " '- ',\n",
       " ' }',\n",
       " ' }',\n",
       " ' <']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt = df[\"modified\"][0][0].content\n",
    "cnt2 = df[\"modified\"][0][1].content\n",
    "diffr = difflib.unified_diff(cnt, cnt2, lineterm='')    # # return diff\n",
    "[str(d) for d in diffr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "revids = [(x[\"revid\"], x[\"parentid\"]) for x in loaded_content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_content[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def convert_wikitext_to_html(wikitext):\n",
    "    \"\"\"\n",
    "    Convert basic wikitext markup to HTML.\n",
    "    \n",
    "    Handles bold, italic, and new line conversions.\n",
    "    \"\"\"\n",
    "    # Convert bold and italic (''''text'''' to <b><i>text</i></b>)\n",
    "    html = re.sub(r\"'''''(.*?)'''''\", r\"<b><i>\\1</i></b>\", wikitext)\n",
    "    # Convert bold ('''text''' to <b>text</b>)\n",
    "    html = re.sub(r\"'''(.*?)'''\", r\"<b>\\1</b>\", html)\n",
    "    # Convert italic (''text'' to <i>text</i>)\n",
    "    html = re.sub(r\"''(.*?)''\", r\"<i>\\1</i>\", html)\n",
    "    # Convert new lines to <br>\n",
    "    html = html.replace(\"\\n\", \"<br>\")\n",
    "\n",
    "    return html\n",
    "\n",
    "def save_to_html_file(text, filename):\n",
    "    \"\"\"\n",
    "    Save the provided text as an HTML file.\n",
    "\n",
    "    :param text: The text to be converted and saved.\n",
    "    :param filename: The name of the file to save the HTML content in.\n",
    "    \"\"\"\n",
    "    html_content = convert_wikitext_to_html(text)\n",
    "    html_full_content = f\"<html><head><title>Wikipedia Page</title></head><body>{html_content}</body></html>\"\n",
    "\n",
    "    # Write the HTML content to a file\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(html_full_content)\n",
    "\n",
    "# Example usage\n",
    "wikitext = \"'''Bold''' and ''italic'' text.\\nNew line here.\"\n",
    "filename = \"ExampleWikipediaPage.html\"\n",
    "save_to_html_file(wikitext, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"test.html\"\n",
    "save_to_html_file(res[3]['*'], filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [{\"revid\": r[\"revid\"], \"parentid\": r[\"parentid\"], \"comment\": r[\"comment\"]} for r in res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Short Description, Level: 0, Content: Example Article\n",
      "\n",
      "Title: Summary, Level: 1, Content: SummaryBlabla\n",
      "\n",
      "Title: Section 1, Level: 2, Content:  Section 1 Content of section 1\n",
      "\n",
      "Title: Subsection, Level: 3, Content:  Subsection Content of subsection.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def extract_sections(wikitext):\n",
    "    sections = {}\n",
    "    # Extracting the short description\n",
    "    short_desc_match = re.search(r\"{{short description\\|(.*?)}}\", wikitext)\n",
    "    short_description = short_desc_match.group(1) if short_desc_match else \"\"\n",
    "    sections[\"Short Description\"] = {\"level\": 0, \"content\": short_description}\n",
    "\n",
    "    # Regular expression for section titles\n",
    "    pattern = r\"(={2,})([^=]+)\\1\"\n",
    "\n",
    "    # Find the start position of the first section header\n",
    "    first_section_match = re.search(pattern, wikitext)\n",
    "    summary_end = first_section_match.start() if first_section_match else len(wikitext)\n",
    "    summary_start = short_desc_match.end() if short_desc_match else 0\n",
    "    sections[\"Summary\"] = {\"level\": 1, \"content\": wikitext[summary_start:summary_end].strip()}\n",
    "\n",
    "    # Splitting the text based on section titles\n",
    "    parts = re.split(pattern, wikitext)\n",
    "\n",
    "    # Iterate over the rest of the parts to extract sections\n",
    "    for i in range(1, len(parts), 3):\n",
    "        level = len(parts[i])  # The number of '=' indicates the level of the section\n",
    "        title = parts[i + 1].strip()\n",
    "        content = parts[i + 1] + parts[i + 2].strip()  # Include the section title in the content\n",
    "        sections[title] = {\"level\": level, \"content\": content}\n",
    "\n",
    "    return sections\n",
    "\n",
    "\n",
    "wikitext_example = \"{{short description|Example Article}}SummaryBlabla\\n== Section 1 ==\\nContent of section 1\\n=== Subsection ===\\nContent of subsection.\"\n",
    "\n",
    "sections = extract_sections(wikitext_example)\n",
    "for title, details in sections.items():\n",
    "    # print(title)\n",
    "    # print(details)\n",
    "    print(f\"Title: {title}, Level: {details.get('level', 'Summary')}, Content: {details['content']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Short Description, Level: 0, Content: Type of mathematical\n",
      "\n",
      "Title: Summary, Level: 1, Content: '''Compartmental mod\n",
      "\n",
      "Title: The SIR model, Level: 3, Content:  The SIR model [[Fil\n",
      "\n",
      "Title: Transition rates, Level: 3, Content: Transition ratesFor \n",
      "\n",
      "Title: The SIR model without birth and death, Level: 3, Content: The SIR model withou\n",
      "\n",
      "Title: The force of infection, Level: 4, Content: The force of infecti\n",
      "\n",
      "Title: Exact analytical solutions to the SIR model, Level: 4, Content: Exact analytical sol\n",
      "\n",
      "Title: Numerical solutions to the SIR model with approximations, Level: 4, Content: Numerical solutions \n",
      "\n",
      "Title: The SIR model with vital dynamics and constant population, Level: 3, Content: The SIR model with v\n",
      "\n",
      "Title: Steady-state solutions, Level: 3, Content:  Steady-state soluti\n",
      "\n",
      "Title: Other compartmental models, Level: 3, Content:  Other compartmental\n",
      "\n",
      "Title: Variations on the basic SIR model, Level: 2, Content: Variations on the ba\n",
      "\n",
      "Title: The SIS model, Level: 3, Content: The SIS model[[File:\n",
      "\n",
      "Title: The SIRD model, Level: 3, Content: The SIRD model[[File\n",
      "\n",
      "Title: The SIRV model, Level: 3, Content: The SIRV modelThe ''\n",
      "\n",
      "Title: The MSIR model, Level: 3, Content: The MSIR modelFor ma\n",
      "\n",
      "Title: Carrier state, Level: 3, Content: Carrier stateSome pe\n",
      "\n",
      "Title: The SEIR model, Level: 3, Content: The SEIR modelFor ma\n",
      "\n",
      "Title: The SEIS model, Level: 3, Content:  The SEIS model The \n",
      "\n",
      "Title: The MSEIR model, Level: 3, Content:  The MSEIR model For\n",
      "\n",
      "Title: The MSEIRS model, Level: 3, Content:  The MSEIRS model An\n",
      "\n",
      "Title: Variable contact rates, Level: 3, Content: Variable contact rat\n",
      "\n",
      "Title: SIR model with diffusion, Level: 3, Content: SIR model with diffu\n",
      "\n",
      "Title: Interacting Subpopulation SEIR Model, Level: 3, Content: Interacting Subpopul\n",
      "\n",
      "Title: SIR Model on Networks, Level: 3, Content:  SIR Model on Networ\n",
      "\n",
      "Title: SIR<sub>SS</sub> model - combination of SIR with modelling of social stress, Level: 3, Content:  SIR<sub>SS</sub> mo\n",
      "\n",
      "Title: The KdV-SIR equation, Level: 3, Content:  The KdV-SIR equatio\n",
      "\n",
      "Title: Modelling vaccination, Level: 2, Content: Modelling vaccinatio\n",
      "\n",
      "Title: Vaccinating newborns, Level: 3, Content: Vaccinating newborns\n",
      "\n",
      "Title: Vaccination and information, Level: 3, Content: Vaccination and info\n",
      "\n",
      "Title: Vaccination of non-newborns, Level: 3, Content: Vaccination of non-n\n",
      "\n",
      "Title: Pulse vaccination strategy, Level: 3, Content: Pulse vaccination st\n",
      "\n",
      "Title: The influence of age: age-structured models, Level: 2, Content: The influence of age\n",
      "\n",
      "Title: Other considerations within compartmental epidemic models, Level: 2, Content:  Other consideration\n",
      "\n",
      "Title: Vertical transmission, Level: 3, Content:  Vertical transmissi\n",
      "\n",
      "Title: Vector transmission, Level: 3, Content:  Vector transmission\n",
      "\n",
      "Title: Others, Level: 3, Content:  Others Other occurr\n",
      "\n",
      "Title: Deterministic versus stochastic epidemic models, Level: 2, Content: Deterministic versus\n",
      "\n",
      "Title: See also, Level: 2, Content:  See also *[[Mathema\n",
      "\n",
      "Title: References, Level: 2, Content:  References {{reflis\n",
      "\n",
      "Title: Further reading, Level: 2, Content:  Further reading * {\n",
      "\n",
      "Title: External links, Level: 2, Content:  External links * [h\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sections = extract_sections(loaded_content[0]['*'])\n",
    "for title, details in sections.items():\n",
    "    content = details.get('content','')\n",
    "    print(f\"Title: {title}, Level: {details.get('level', 'Summary')}, Content: {content[0:min(20, len(content))]}\\n\")\n",
    "# loaded_content[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import difflib\n",
    "from typing import Mapping, List\n",
    "\n",
    "\n",
    "class Part:\n",
    "    def __init__(self, title, super_title, level, header, headerless_content) -> None:\n",
    "        self.title = title\n",
    "        self.super_title =  super_title\n",
    "        self.level = level\n",
    "        self.header = header\n",
    "        self.headerless_content = headerless_content\n",
    "\n",
    "    @property\n",
    "    def content(self):\n",
    "        header = f\"{self.header}\\n\" if self.header else \"\"\n",
    "        return header + self.headerless_content\n",
    "\n",
    "\n",
    "class WikipediaStructuredText():\n",
    "\n",
    "    def __init__(self, wikitext):\n",
    "        self.wikitext = wikitext\n",
    "\n",
    "    # get a property called parts\n",
    "    @property\n",
    "    def parts(self):\n",
    "        return self.extract_parts()\n",
    "\n",
    "    def extract_parts(self) -> List[Part]:\n",
    "        sections = list()\n",
    "        section_stack = list()\n",
    "        # Extracting the short description\n",
    "        short_desc_match = re.search(r\"{{short description\\|(.*?)}}\", self.wikitext)\n",
    "        short_description = short_desc_match.group(1) if short_desc_match else \"\"\n",
    "        sections.append(Part(title=\"Short Description\", super_title=\"\", level=0, header=\"\", headerless_content=short_description))\n",
    "\n",
    "        # Regular expression for section titles\n",
    "        pattern = r\"(={2,})([^=]+)\\1\"\n",
    "\n",
    "        # Find the start position of the first section header\n",
    "        first_section_match = re.search(pattern, self.wikitext)\n",
    "        summary_end = first_section_match.start() if first_section_match else len(self.wikitext)\n",
    "        summary_start = short_desc_match.end() if short_desc_match else 0\n",
    "        sections.append(Part(title=\"Summary\", super_title=\"\", level=1, headerless_content= self.wikitext[summary_start:summary_end].strip(), header=\"\"))\n",
    "\n",
    "        # Splitting the text based on section titles\n",
    "        parts = re.split(pattern, self.wikitext)\n",
    "        \n",
    "        for i in range(1, len(parts), 3):\n",
    "            level = len(parts[i])  # The number of '=' indicates the level of the section\n",
    "            title = parts[i + 1].strip()\n",
    "            headerless_content = parts[i + 2].strip()\n",
    "            super_title = \"\"\n",
    "\n",
    "            # Update the stack and determine the super_title\n",
    "            while section_stack and section_stack[-1].level >= level:\n",
    "                section_stack.pop()\n",
    "            if section_stack:\n",
    "                super_title = section_stack[-1].title\n",
    "\n",
    "            # Add the current section to the stack\n",
    "            current_part = Part(title=title, level=level, header=title, headerless_content=headerless_content, super_title=super_title)\n",
    "            section_stack.append(current_part)\n",
    "\n",
    "            # Add the section to the list of sections\n",
    "            sections.append(current_part)\n",
    "\n",
    "        return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "text1 = WikipediaStructuredText(wikitext=loaded_content[31]['*'])\n",
    "text2 = WikipediaStructuredText(wikitext=loaded_content[32]['*'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1_parts = text1.parts\n",
    "# text1_parts[3].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fine_grained_diff(text1, text2):\n",
    "sections1 = text1.parts\n",
    "sections2 = text2.parts\n",
    "\n",
    "diffs = {\n",
    "    \"structure_diff\": [],\n",
    "    \"header_diff\": [],\n",
    "    \"content_diff\": {}\n",
    "}\n",
    "\n",
    "# Compare structure (section presence)\n",
    "set1 = set(sections1.keys())\n",
    "set2 = set(sections2.keys())\n",
    "diffs[\"structure_diff\"].extend(list(set1 - set2))  # Present in text1 but not in text2\n",
    "diffs[\"structure_diff\"].extend(list(set2 - set1))  # Present in text2 but not in text1\n",
    "\n",
    "# Compare headers and short description\n",
    "for section in set1.union(set2):\n",
    "    if sections1.get(section) != sections2.get(section):\n",
    "        diffs[\"header_diff\"].append(section)\n",
    "\n",
    "# Compare content of each section\n",
    "for section in set1.intersection(set2):\n",
    "    content_diff = list(difflib.unified_diff(\n",
    "        sections1[section][\"content\"].splitlines(), \n",
    "        sections2[section][\"content\"].splitlines(),\n",
    "        lineterm=''\n",
    "    ))\n",
    "    if content_diff:\n",
    "        diffs[\"content_diff\"][section] = content_diff\n",
    "\n",
    "# return diffs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'deleted': [],\n",
       " 'added': [],\n",
       " 'rearranged': [],\n",
       " 'modified': [(<__main__.Part at 0x7fccd7585280>,\n",
       "   <__main__.Part at 0x7fccd75b9fa0>),\n",
       "  (<__main__.Part at 0x7fccd7585a00>, <__main__.Part at 0x7fccd76296a0>),\n",
       "  (<__main__.Part at 0x7fccd7585a30>, <__main__.Part at 0x7fccd76292e0>)]}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_similar(part1, part2, similarity_threshold=0.6):\n",
    "    \"\"\"\n",
    "    Determines if two parts are similar based on their title, level, super_title, and content.\n",
    "\n",
    "    :param part1: First part to compare.\n",
    "    :param part2: Second part to compare.\n",
    "    :param similarity_threshold: Threshold for content similarity.\n",
    "    :return: Boolean indicating if the parts are similar.\n",
    "    \"\"\"\n",
    "    # Check if levels are different\n",
    "    if part1.level != part2.level:\n",
    "        return False\n",
    "\n",
    "    # Check if titles and super_titles are the same\n",
    "    if part1.title == part2.title and part1.super_title == part2.super_title:\n",
    "        return True\n",
    "\n",
    "    # Check if titles are different but contents are similar\n",
    "    if part1.title != part2.title and part1.super_title == part2.super_title:\n",
    "        return _similar_content(part1.content, part2.content) >= similarity_threshold\n",
    "\n",
    "    # Check if super_titles are different but titles are the same and contents are similar\n",
    "    if part1.super_title != part2.super_title and part1.title == part2.title:\n",
    "        return _similar_content(part1.content, part2.content) >= similarity_threshold\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "import difflib\n",
    "\n",
    "def _similar_content(content1, content2):\n",
    "    \"\"\"\n",
    "    Checks if the content of two parts is similar using difflib's SequenceMatcher.\n",
    "\n",
    "    :param content1: Content of the first part.\n",
    "    :param content2: Content of the second part.\n",
    "    :return: Similarity ratio (float) between the contents.\n",
    "    \"\"\"\n",
    "    # Create a SequenceMatcher object\n",
    "    matcher = difflib.SequenceMatcher(None, content1, content2)\n",
    "\n",
    "    # The ratio is a measure of the sequences' similarity\n",
    "    return matcher.ratio()\n",
    "\n",
    "\n",
    "# Helper function to find a similar part\n",
    "def find_similar_part_in_second(part_1, part_2_s, similarity_threshold=0.6):\n",
    "    for i, other_part in enumerate(part_2_s):\n",
    "        if is_similar(part_1, other_part, similarity_threshold=similarity_threshold):\n",
    "            return other_part, i\n",
    "    return None, -1\n",
    "\n",
    "# Helper function to find a similar part\n",
    "def find_similar_part_in_first(part_1_s, part_2, similarity_threshold=0.6):\n",
    "    for i, other_part in enumerate(part_1_s):\n",
    "        if is_similar(other_part, part_2, similarity_threshold=similarity_threshold):\n",
    "            return other_part, i\n",
    "    return None, -1\n",
    "\n",
    "\n",
    "def analyze_differences(text1_parts, text2_parts, similarity_threshold=0.6):\n",
    "    deleted, added, rearranged, modified = [], [], [], []\n",
    "\n",
    "\n",
    "    # Identify deleted and added parts\n",
    "    for p in text1_parts:\n",
    "        q, i = find_similar_part_in_second(p, text2_parts)\n",
    "        if not q:\n",
    "            deleted.append(p)\n",
    "    for q in text2_parts:\n",
    "        p, i = find_similar_part_in_first(text1_parts, q)\n",
    "        if not p:\n",
    "            added.append(q)\n",
    "\n",
    "    # Identify rearranged and modified parts\n",
    "    for i, p in enumerate(text1_parts):\n",
    "        # print the title and the first 20 characters of the headerless content (save first the headerless content in a local variable)\n",
    "        hl_content_p = p.headerless_content\n",
    "        hl_content_q = q.headerless_content\n",
    "        \n",
    "        q, j = find_similar_part_in_second(p, text2_parts)\n",
    "        if q:\n",
    "            if i != j:\n",
    "                rearranged.append((i, j))\n",
    "            if p.content != q.content:\n",
    "                modified.append((p, q))\n",
    "\n",
    "    return {\"deleted\": deleted, \"added\": added, \"rearranged\": rearranged, \"modified\": modified}\n",
    "\n",
    "# Example usage\n",
    "result = analyze_differences(text1.parts, text2.parts)\n",
    "result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--- ', '+++ ', '@@ -1,73 +1,6 @@', '-The SIR model without birth and death', '-[[File:SIR trajectory.png|thumb|400px|right|A single realization of the SIR epidemic as produced with an implementation of the [[Gillespie algorithm]] and the numerical solution of the ordinary differential equation system (dashed).]]', '-The dynamics of an epidemic, for example, the [[Influenza|flu]], are often much faster than the dynamics of birth and death, therefore, birth and death are often omitted in simple compartmental models.  The SIR system without so-called vital dynamics (birth and death, sometimes called demography) described above can be expressed by the following system of ordinary [[differential equations]]:<ref name=\"Beckley\"/><ref name=\"Hethcote2000\">{{cite journal |author=Hethcote H |title=The Mathematics of Infectious Diseases |journal=SIAM Review |volume=42 |issue= 4|pages=599–653 |year=2000 |doi=10.1137/s0036144500371907|bibcode=2000SIAMR..42..599H |s2cid=10836889 }}</ref>', '+Transition rates', \"+For the full specification of the model, the arrows should be labeled with the transition rates between compartments. Between ''S'' and ''I'', the transition rate is assumed to be ''d(S/N)/dt = -βSI/N<sup>2</sup>'', where ''N'' is the total population, β is the average number of contacts per person per time, multiplied by the probability of disease transmission in a contact between a susceptible and an infectious subject, and ''SI/N<sup>2</sup>'' is the fraction of those contacts between an infectious and susceptible individual which result in the susceptible person becoming infected. (This is mathematically similar to the [[law of mass action]] in chemistry in which random collisions between molecules result in a chemical reaction and the fractional rate is proportional to the concentration of the two reactants).{{cn}}\", ' ', '-:<math>', '-\\\\left\\\\{\\\\begin{aligned}', '-& \\\\frac{dS}{dt} = - \\\\frac{\\\\beta I S}{N}, \\\\\\\\[6pt]', '-& \\\\frac{dI}{dt} = \\\\frac{\\\\beta I S}{N}- \\\\gamma I, \\\\\\\\[6pt]', '-& \\\\frac{dR}{dt} = \\\\gamma I,', '-\\\\end{aligned}\\\\right.', '-</math>', '-[[File:SIR model cartoon.png|thumb|right|The SIR model.]]', '+Between \\'\\'I\\'\\' and \\'\\'R\\'\\', the transition rate is assumed to be proportional to the number of infectious individuals which is γ\\'\\'I\\'\\'. This is equivalent to assuming that the probability of an infectious individual recovering in any time interval \\'\\'dt\\'\\' is simply γ\\'\\'dt\\'\\'. If an individual is infectious for an average time period \\'\\'D\\'\\', then γ = 1/\\'\\'D\\'\\'. This is also equivalent to the assumption that the length of time spent by an individual in the infectious state is a random variable with an [[exponential distribution]]. The \"classical\" SIR model may be modified by using more complex and realistic distributions for the I-R transition rate (e.g. the [[Erlang distribution]]<ref name=\"Krylova2013\">{{cite journal | vauthors = Krylova O, Earn DJ | title = Effects of the infectious period distribution on predicted transitions in childhood disease dynamics | journal = Journal of the Royal Society, Interface | volume = 10 | issue = 84 | pages = 20130098 | date = July 2013 | pmid = 23676892 | pmc = 3673147 | doi = 10.1098/rsif.2013.0098 | doi-access = free }}</ref>).', ' ', '-where <math>S</math> is the stock of susceptible population, <math>I</math> is the stock of infected, <math>R</math> is the stock of removed population (either by death or recovery), and <math>N</math> is the sum of these three.', '-', '-This model was for the first time proposed by [[William Ogilvy Kermack]] and [[Anderson Gray McKendrick]] as a special case of what we now call [[Kermack–McKendrick theory]], and followed work McKendrick had done with [[Ronald Ross]].{{cn|date=May 2023}}', '-', '-This system is [[non-linear]], however it is possible to derive its analytic solution in implicit form.<ref name=\"Harko\"/> Firstly note that from:', '-', '-:<math> \\\\frac{dS}{dt} + \\\\frac{dI}{dt} + \\\\frac{dR}{dt}  = 0,</math>', '-', '-it follows that:', '-', '-:<math> S(t) + I(t) + R(t) = \\\\text{constant} = N,</math>', '-', '-expressing in mathematical terms the constancy of population <math> N </math>. Note that the above relationship implies that one need only study the equation for two of the three variables.', '-', '-Secondly, we note that the dynamics of the infectious class depends on the following ratio:', '-', '-:<math> R_0 = \\\\frac{\\\\beta}{\\\\gamma},</math>', '-', \"-the so-called [[basic reproduction number]] (also called basic reproduction ratio). This ratio is derived as the expected number of new infections (these new infections are sometimes called secondary infections) from a single infection in a population where all subjects are susceptible.<ref name=Bailey1975>{{cite book |author=Bailey, Norman T. J. |title=The mathematical theory of infectious diseases and its applications |publisher=Griffin |location=London |year=1975 |isbn=0-85264-231-8 |edition=2nd}}</ref><ref name=nunn2006>{{cite book |author1=Sonia Altizer |author2=Nunn, Charles |title=Infectious diseases in primates: behavior, ecology and evolution |publisher=Oxford University Press |location=Oxford [Oxfordshire] |year=2006 |isbn=0-19-856585-2 |series=Oxford Series in Ecology and Evolution}}</ref> This idea can probably be more readily seen if we say that the typical time between contacts is <math>T_{c} = \\\\beta^{-1}</math>, and the typical time until removal is <math>T_{r} = \\\\gamma^{-1}</math>. From here it follows that, on average, the number of contacts by an infectious individual with others ''before'' the infectious has been removed is: <math>T_{r}/T_{c}.</math>\", '-', '-By dividing the first differential equation by the third, [[Separation of variables|separating the variables]] and integrating we get', '-', '-:<math> S(t) = S(0) e^{-R_0(R(t) - R(0))/N}, </math>', '-', '-where <math>S(0)</math> and <math>R(0)</math> are the initial numbers of, respectively, susceptible and removed subjects. ', '-Writing <math>s_0 = S(0) / N</math> for the initial proportion of susceptible individuals, and', '-<math>s_\\\\infty = S(\\\\infty) / N</math> and ', '-<math>r_\\\\infty = R(\\\\infty) / N</math> for the proportion of susceptible and removed individuals respectively', '-in the limit <math>t \\\\to \\\\infty,</math> one has', '-', '-:<math>s_\\\\infty = 1 - r_\\\\infty = s_0 e^{-R_0(r_\\\\infty - r_0)}</math>', '-', '-(note that the infectious compartment empties in this limit).', '-This [[transcendental equation]] has a solution in terms of the [[Lambert W function|Lambert {{mvar|W}} function]],<ref>{{cite web |author1=Wolfram Research, Inc. |title=Mathematica, Version 12.1 |url=https://www.wolfram.com/mathematica |publisher=Champaign IL, 2020}}</ref> namely', '-', '-:<math>s_\\\\infty = 1-r_\\\\infty = - R_0^{-1}\\\\, W(-s_0 R_0 e^{-R_0(1-r_0)}).</math>', '-', '-This shows that at the end of an epidemic that conforms to the simple assumptions of the SIR model, unless <math>s_0=0</math>, not all individuals of the population have been removed, so some must remain susceptible. A driving force leading to the end of an epidemic is a decline in the number of infectious individuals. The epidemic does not typically end because of a complete lack of susceptible individuals.', '-', '-The role of both the [[basic reproduction number]] and the initial susceptibility are extremely important. In fact, upon rewriting the equation for infectious individuals as follows:', '-', '-:<math> \\\\frac{dI}{dt} = \\\\left(R_0 \\\\frac{S}{N}  - 1\\\\right) \\\\gamma I,</math>', '-', '-it yields that if:', '-', '-:<math> R_{0} \\\\cdot S(0) > N,</math>', '-', '-then:', '-', '-:<math> \\\\frac{dI}{dt}(0) >0 ,</math>', '-', '-i.e., there will be a proper epidemic outbreak with an increase of the number of the  infectious (which can reach a considerable fraction of the population). On the contrary, if', '-', '-:<math> R_{0} \\\\cdot S(0) < N,</math>', '-', '-then', '-', '-:<math> \\\\frac{dI}{dt}(0) <0 ,</math>', '-', '-i.e., independently from the initial size of the susceptible population the disease can never cause a proper epidemic outbreak. As a consequence, it is clear that both the [[basic reproduction number]] and the initial susceptibility are extremely important.', '+For the special case in which there is no removal from the infectious compartment (γ=0), the SIR model reduces to a very simple SI model, which has a [[logistic distribution|logistic]] solution, in which every individual eventually becomes infected.']\n"
     ]
    }
   ],
   "source": [
    "part1 = text1.parts[4]\n",
    "part2 = text2.parts[3]\n",
    "content1 = part1.content\n",
    "content2 = part2.content\n",
    "(_similar_content(part1.content, part2.content),\n",
    "(part1.title, part1.super_title, part1.level, part1.headerless_content[0:min(20, len(part1.headerless_content))]),\n",
    "(part2.title, part2.super_title, part2.level, part2.headerless_content[0:min(20, len(part2.headerless_content))]))\n",
    "\n",
    "diff = list(difflib.unified_diff(content1.splitlines(), content2.splitlines(), lineterm=''))\n",
    "print(diff)\n",
    "diff_lines = sum(1 for line in diff if line.startswith('+ ') or line.startswith('- '))\n",
    "total_lines = max(len(content1.splitlines()), len(content2.splitlines()))\n",
    "\n",
    "# Check if the diff affects more than the similarity threshold\n",
    "# return 1 - diff_lines / total_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--- ', '+++ ', '@@ -1 +1 @@', '-This is the first version of the text.', '+This is the second version of the text.']\n"
     ]
    }
   ],
   "source": [
    "import difflib\n",
    "\n",
    "def compare_texts(text1, text2):\n",
    "    \"\"\"\n",
    "    Generate a diff between two texts.\n",
    "\n",
    "    :param text1: The first text string.\n",
    "    :param text2: The second text string.\n",
    "    :return: The diff between the two texts.\n",
    "    \"\"\"\n",
    "    text1_lines = text1.splitlines()\n",
    "    text2_lines = text2.splitlines()\n",
    "\n",
    "    diff = difflib.unified_diff(text1_lines, text2_lines, lineterm='')\n",
    "\n",
    "    # return diff\n",
    "    return [str(d) for d in diff]\n",
    "\n",
    "# Example usage\n",
    "text_version_1 = \"This is the first version of the text.\"\n",
    "text_version_2 = \"This is the second version of the text.\"\n",
    "diff_result = compare_texts(text_version_1, text_version_2)\n",
    "print(diff_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['--- ',\n",
       " '+++ ',\n",
       " '@@ -9,7 +9,7 @@',\n",
       " ' ',\n",
       " ' ==The SIR model==',\n",
       " ' ',\n",
       " '-The \\'\\'\\'SIR model\\'\\'\\'<ref name=\"Harko\">{{Cite journal| vauthors = Harko T, Lobo FS, Mak MK |s2cid=14509477|title=Exact analytical solutions of the Susceptible-Infected-Recovered (SIR) epidemic model and of the SIR model with equal death and birth rates |journal=Applied Mathematics and Computation|language=en|volume=236|pages=184–194|year=2014 |doi=10.1016/j.amc.2014.03.030|bibcode=2014arXiv1403.2160H |arxiv=1403.2160 }}</ref><ref name=\"Beckley\">{{cite journal | vauthors = Beckley R, Weatherspoon C, Alexander M, Chandler M, Johnson A, Bhatt GS |date=2013 |title=Modeling epidemics with differential equations |url=http://www.tnstate.edu/mathematics/mathreu/filesreu/GroupProjectSIR.pdf |journal=Tennessee State University Internal Report |access-date=July 19, 2020}}</ref><ref name=\"KrogerSchlickeiser\">{{Cite journal| vauthors = Kröger M, Schlickeiser R |s2cid=225555567 |title=Analytical solution of the SIR-model for the temporal evolution of epidemics. Part A: Time-independent reproduction factor|journal=Journal of Physics A|language=en|year=2020|volume=53|issue=50|page=505601|doi=10.1088/1751-8121/abc65d|bibcode=2020JPhA...53X5601K|doi-access=free}}</ref><ref name=\"KrogerSchlickeiser_partB\">{{Cite journal| vauthors = Schlickeiser R, Kröger M |title=Analytical solution of the SIR-model for the temporal evolution of epidemics. Part B: Semi-time case|journal=Journal of Physics A|language=en|year=2021|volume=54|issue=17|page=175601|doi=10.1088/1751-8121/abed66|bibcode=2021JPhA...54X5601S|doi-access=free}}</ref> is one of the simplest compartmental models, and many models are derivatives of this basic form. The model consists of three compartments:',\n",
       " '+The \\'\\'\\'SIR model\\'\\'\\'<ref name=\"Harko\">{{Cite journal| vauthors = Harko T, Lobo FS, Mak MK |s2cid=14509477|title=Exact analytical solutions of the Susceptible-Infected-Recovered (SIR) epidemic model and of the SIR model with equal death and birth rates |journal=Applied Mathematics and Computation|language=en|volume=236|pages=184–194|year=2014 |doi=10.1016/j.amc.2014.03.030|bibcode=2014arXiv1403.2160H |arxiv=1403.2160 }}</ref><ref name=\"Beckley\">{{cite journal | vauthors = Beckley R, Weatherspoon C, Alexander M, Chandler M, Johnson A, Bhatt GS |date=2013 |title=Modeling epidemics with differential equations |url=http://www.tnstate.edu/mathematics/mathreu/filesreu/GroupProjectSIR.pdf |journal=Tennessee State University Internal Report |access-date=July 19, 2020}}</ref><ref name=\"KrogerSchlickeiser\">{{Cite journal| vauthors = Kröger M, Schlickeiser R |s2cid=225555567 |title=Analytical solution of the SIR-model for the temporal evolution of epidemics. Part A: Time-independent reproduction factor|journal=Journal of Physics A|language=en|year=2020|volume=53|issue=50|page=505601|doi=10.1088/1751-8121/abc65d|bibcode=2020JPhA...53X5601K|doi-access=free}}</ref><ref name=\"KrogerSchlickeiser_partB\">{{Cite journal| vauthors = Schlickeiser R, Kröger M |title=Analytical solution of the SIR-model for the temporal evolution of epidemics. Part B: Semi-time case|journal=Journal of Physics A|language=en|year=2021|volume=54|issue=17|page=175601|doi=10.1088/1751-8121/abed66|bibcode=2021JPhA...54q5601S|doi-access=free}}</ref> is one of the simplest compartmental models, and many models are derivatives of this basic form. The model consists of three compartments:',\n",
       " ' :\\'\\'\\'S\\'\\'\\': The number of \\'\\'\\'s\\'\\'\\'usceptible individuals. When a susceptible and an infectious individual come into \"infectious contact\", the susceptible individual contracts the disease and transitions to the infectious compartment.',\n",
       " \" :'''I''': The number of '''i'''nfectious individuals. These are individuals who have been infected and are capable of infecting susceptible individuals.\",\n",
       " ' :\\'\\'\\'R\\'\\'\\' for the number of \\'\\'\\'r\\'\\'\\'emoved (and immune) or deceased individuals. These are individuals who have been infected and have either recovered from the disease and entered the removed compartment, or died. It is assumed that the number of deaths is negligible with respect to the total population. This compartment may also be called \"\\'\\'\\'r\\'\\'\\'ecovered\"  or \"\\'\\'\\'r\\'\\'\\'esistant\".']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_texts(res[15]['*'], res[14]['*'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
